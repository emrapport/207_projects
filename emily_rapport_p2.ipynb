{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, stripping out metadata so that we learn classifiers that only use textual features. By default, newsgroups data is split into train and test sets. We further split the test so we have a dev set. Note that we specify 4 categories to use for this project. If you remove the categories argument from the fetch function, you'll get all 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[int(num_test/2):], newsgroups_test.target[int(num_test/2):]\n",
    "dev_data, dev_labels = newsgroups_test.data[:int(num_test/2)], newsgroups_test.target[:int(num_test/2)]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('test label shape:', test_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) For each of the first 5 training examples, print the text of the message along with the label.\n",
    "\n",
    "[2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P1(num_examples=5):\n",
    "### STUDENT START ###\n",
    "\n",
    "## TODO: print label names instead of just ints\n",
    "for i in range(5):\n",
    "    print(\"\\n\\nTraining Example {}:\".format(i))\n",
    "    print(\"\\n{}\".format(train_data[i]))\n",
    "    print(\"\\nLabel: {}\".format(newsgroups_train.target_names[train_labels[i]]))\n",
    "\n",
    "### STUDENT END ###\n",
    "#P1(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Use CountVectorizer to turn the raw training text into feature vectors. You should use the fit_transform function, which makes 2 passes through the data: first it computes the vocabulary (\"fit\"), second it converts the raw text into feature vectors using the vocabulary (\"transform\").\n",
    "\n",
    "The vectorizer has a lot of options. To get familiar with some of them, write code to answer these questions:\n",
    "\n",
    "a. The output of the transform (also of fit_transform) is a sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html. What is the size of the vocabulary? What is the average number of non-zero features per example? What fraction of the entries in the matrix are non-zero? Hint: use \"nnz\" and \"shape\" attributes.\n",
    "\n",
    "b. What are the 0th and last feature strings (in alphabetical order)? Hint: use the vectorizer's get_feature_names function.\n",
    "\n",
    "c. Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. Confirm the training vectors are appropriately shaped. Now what's the average number of non-zero features per example?\n",
    "\n",
    "d. Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents. What size vocabulary does this yield?\n",
    "\n",
    "f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
    "\n",
    "[6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def P2():\n",
    "### STUDENT START ###\n",
    "\n",
    "# these are the default\n",
    "# will use them in subsequent problems\n",
    "# that ask for default CountVectorizer\n",
    "def make_cv_get_dtms(cv, train_data, dev_data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    doc_term_matrix = count_vectorizer.fit_transform(train_data)\n",
    "    dev_doc_term = count_vectorizer.transform(dev_data)\n",
    "    return count_vectorizer, doc_term_matrix, dev_doc_term\n",
    "\n",
    "(default_cv,\n",
    " default_train_dtm,\n",
    " default_dev_dtm) = make_cv_get_dtms(CountVectorizer(), train_data, dev_data)\n",
    "\n",
    "## QUESTION A:\n",
    "print(\"Question A:\")\n",
    "print(\"Vocabulary size: {}\".format(default_train_dtm.shape[1]))\n",
    "print(\"Average number of non-zero features per example: {:.2f}\".format(default_train_dtm.nnz / \n",
    "                                                                       default_train_dtm.shape[0]))\n",
    "print(\"Fraction of entries in matrix that are non-zero: {:.4f}\".format(default_train_dtm.nnz /\n",
    "                                                                       (default_train_dtm.shape[0] * \n",
    "                                                                       default_train_dtm.shape[1])))\n",
    "\n",
    "##QUESTION B:\n",
    "print(\"\\nQuestion B:\")\n",
    "feature_names = default_cv.get_feature_names()\n",
    "print(\"First feature name: {}\".format(feature_names[0]))\n",
    "print(\"Last feature name: {}\".format(feature_names[-1]))\n",
    "\n",
    "##QUESTION C:\n",
    "print(\"\\nQuestion C:\")\n",
    "vocab_specific_count_vectorizer = CountVectorizer(vocabulary=[\"atheism\", \"graphics\", \"space\", \"religion\"])\n",
    "# Confirm the training vectors are appropriately shaped\n",
    "vocab_specific_doc_term_matrix = vocab_specific_count_vectorizer.fit_transform(train_data)\n",
    "print(\"Training vector shape: {}\".format(vocab_specific_doc_term_matrix.shape))\n",
    "print(\"Average number of non-zero features per example: {:.2f}\".format(vocab_specific_doc_term_matrix.nnz / \n",
    "                                                                   vocab_specific_doc_term_matrix.shape[0]))\n",
    "print(\"Fraction of entries in matrix that are non-zero: {:.4f}\".format(vocab_specific_doc_term_matrix.nnz /\n",
    "                                                                       (vocab_specific_doc_term_matrix.shape[0] * \n",
    "                                                                       vocab_specific_doc_term_matrix.shape[1])))\n",
    "\n",
    "##QUESTION D:\n",
    "print(\"\\nQuestion D:\")\n",
    "bigram_cv = CountVectorizer(analyzer='char', ngram_range=(2,2))\n",
    "bigram_doc_term_matrix = bigram_cv.fit_transform(train_data)\n",
    "print(\"Vocabulary size (for bi-gram character features): {}\".format(bigram_doc_term_matrix.shape[1]))\n",
    "\n",
    "trigram_cv = CountVectorizer(analyzer='char', ngram_range=(3,3))\n",
    "trigram_doc_term_matrix = trigram_cv.fit_transform(train_data)\n",
    "print(\"Vocabulary size (for tri-gram character features): {}\".format(trigram_doc_term_matrix.shape[1]))\n",
    "\n",
    "\n",
    "##QUESTION E:\n",
    "print(\"\\nQuestion E:\")\n",
    "min_word_cv = CountVectorizer(min_df=10)\n",
    "min_word_doc_term_matrix = min_word_cv.fit_transform(train_data)\n",
    "print(\"Vocabulary size (with only words that occur 10+ times): {}\".format(min_word_doc_term_matrix.shape[1]))\n",
    "\n",
    "##Question F:\n",
    "print(\"\\nQuestion F:\")\n",
    "# build a doc term matrix that includes train and dev\n",
    "cv = CountVectorizer()\n",
    "cv_doc_term_matrix = cv.fit_transform(train_data + dev_data)\n",
    "\n",
    "# also build one that includes just dev\n",
    "dev_cv = CountVectorizer()\n",
    "dev_cv_doc_term_matrix = dev_cv.fit_transform(dev_data)\n",
    "\n",
    "# subtract length of train only from train and dev\n",
    "num_dev_words_not_in_train = cv_doc_term_matrix.shape[1] - default_train_dtm.shape[1]\n",
    "\n",
    "# take that over length of dev\n",
    "dev_fraction_missing = num_dev_words_not_in_train / dev_cv_doc_term_matrix.shape[1]\n",
    "print(\"Fraction of dev words missing from train set: {:.2f}\".format(dev_fraction_missing))\n",
    "### STUDENT END ###\n",
    "#P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Use the default CountVectorizer options and report the f1 score (use metrics.f1_score) for a k nearest neighbors classifier; find the optimal value for k. Also fit a Multinomial Naive Bayes model and find the optimal value for alpha. Finally, fit a logistic regression model and find the optimal value for the regularization strength C using l2 regularization. A few questions:\n",
    "\n",
    "a. Why doesn't nearest neighbors work well for this problem?\n",
    "\n",
    "b. Any ideas why logistic regression doesn't work as well as Naive Bayes?\n",
    "\n",
    "c. Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P3():\n",
    "### STUDENT START ###\n",
    "def fit_grid_search(model, \n",
    "                    params, \n",
    "                    dtm,\n",
    "                    dev_dtm,\n",
    "                    model_name):\n",
    "    clf = GridSearchCV(model, params, cv=3)\n",
    "    clf.fit(dtm, train_labels)\n",
    "    print(\"\\nBest params: {}\".format(clf.best_params_))    \n",
    "    y_pred = clf.predict(dev_dtm)\n",
    "    f1 = metrics.f1_score(dev_labels, \n",
    "                          y_pred,\n",
    "                          average=\"weighted\")\n",
    "    print(\"Score for best {} classifier: {:.3f}\".format(model_name,\n",
    "                                                    f1))\n",
    "    return clf, f1\n",
    "\n",
    "\n",
    "parameters = {'n_neighbors': list(range(1, 100, 2)), 'weights':['distance', 'uniform']}\n",
    "knn = KNeighborsClassifier()\n",
    "_ = fit_grid_search(knn,\n",
    "                parameters,\n",
    "                default_train_dtm,\n",
    "                default_dev_dtm,\n",
    "                \"knn\")\n",
    "\n",
    "# using a geometric sequence to try to efficiently get in the right ballpark\n",
    "parameters = {'alpha': [2 * .5 ** (n - 1) for n in range(1, 15 + 1)]}\n",
    "m_nb = MultinomialNB()\n",
    "_ = fit_grid_search(m_nb, \n",
    "                parameters,\n",
    "                default_train_dtm,\n",
    "                default_dev_dtm,\n",
    "                \"multinomial nb\")\n",
    "\n",
    "parameters = {'C': [2 * .5 ** (n - 1) for n in range(1, 15 + 1)] }\n",
    "log_reg = LogisticRegression(penalty='l2')\n",
    "_ = fit_grid_search(log_reg,\n",
    "                parameters,\n",
    "                default_train_dtm,\n",
    "                default_dev_dtm,\n",
    "                \"logistic regression\")\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "#P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWERS:\n",
    "\n",
    "*a. Why doesn't nearest neighbors work well for this problem? *\n",
    "To predict using a nearest neighbors classifier for this problem, the prediction is made by looking to see which other documents have the most similar features (which other documents have the most words in common with the document we're predicting for?). That method is likely prone to error, because a given document could use a totally different vocabulary than a related document, even though they're describing the same topic. Because nearest neighbors is non-parametric, the algorithm equally weights all features in measuring distance, so in deciding which documents are neighbors, it will treat similarities in meaningless, common words (which are likely to occur a lot) just as importantly as similarities in rarer, signal words. \n",
    "\n",
    "\n",
    "### todo: think about bias and variance more - lots of variance \n",
    "b. *Any ideas why logistic regression doesn't work as well as Naive Bayes?*\n",
    "Naive Bayes works particularly well for this problem because the independence assumption made in Naive Bayes is well-suited to this problem. The independence assumption allows the model to treat each word in the document completely independently in determining its probability, which is a helpful assumption when you have a large number of words and those words may or may not co-occur in similar document. In logistic regression, the model learns coefficients that explain the additional likelihood that a document is of a particular class _given that all other words in the document are equal_. This is a more restrictive assumption that isn't as well-suited to this problem. \n",
    "\n",
    "c. *Logistic regression estimates a weight vector for each class, which you can access with the coef\\_ attribute. Output the sum of the squared weight values for each class for each setting of the C parameter. Briefly explain the relationship between the sum and the value of C.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "sswvs = []\n",
    "for c in [2 * .5 ** (n - 1) for n in range(1, 15 + 1)]:\n",
    "    log_reg = LogisticRegression(C=c)\n",
    "    log_reg.fit(default_train_dtm, train_labels)\n",
    "    wvs = log_reg.coef_\n",
    "    sum_square_wvs = np.sum(np.square(log_reg.coef_))\n",
    "    print(\"\\nC: {}\".format(c))\n",
    "    cs.append(c)\n",
    "    print(\"Sum of squared weight values: {}\".format(sum_square_wvs))\n",
    "    sswvs.append(sum_square_wvs)\n",
    "    \n",
    "plt.plot(cs, sswvs, '.')\n",
    "plt.title(\"Sum of Squared Weight vs C values\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Sum of Squared Weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight values get larger as the value of C gets higher. Smaller C values cause stronger regularization. Regularization penalizes the model for getting more complicated by causing the loss value to penalize large parameters. It makes sense that when we use less strong regularization, there would be more non-zero and large weights. Indeed, we see the aggregated size of the weights get larger as C gets larger (regularization penalty gets more relaxed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train a logistic regression model. Find the 5 features with the largest weights for each label -- 20 features in total. Create a table with 20 rows and 4 columns that shows the weight for each of these features for each of the labels. Create the table again with bigram features. Any surprising features in this table?\n",
    "\n",
    "[5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alt version where I use pandas\n",
    "#def P4():\n",
    "### STUDENT START ###\n",
    "\n",
    "### TODO: clean this up a lot\n",
    "def make_coefs_table(d_t_m, cv):\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(d_t_m, train_labels)\n",
    "    idx_to_word = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    # I'm going to interpret largest weights to mean largest absolute value\n",
    "    # because that corresponds best to the idea of which features are strongest signals\n",
    "    # for or against a class \n",
    "    coefs = log_reg.coef_\n",
    "    all_idxes_of_interest = []\n",
    "    cat_labels_for_df = []\n",
    "    for i, coef_set in enumerate(coefs):\n",
    "        sorted_coefs = sorted(coef_set, key=abs, reverse=True)\n",
    "        top_5 = sorted_coefs[:5]\n",
    "        indexes = [np.where(coef_set==big)[0][0] for big in top_5]\n",
    "        all_idxes_of_interest.extend(indexes)\n",
    "        cat_labels_for_df.extend([newsgroups_train.target_names[i]]*5)\n",
    "\n",
    "    words = [idx_to_word[idx] for idx in all_idxes_of_interest]\n",
    "    coef_0_list = [coefs[0, idx] for idx in all_idxes_of_interest]\n",
    "    coef_1_list = [coefs[1, idx] for idx in all_idxes_of_interest]\n",
    "    coef_2_list = [coefs[2, idx] for idx in all_idxes_of_interest]\n",
    "    coef_3_list = [coefs[3, idx] for idx in all_idxes_of_interest]\n",
    "\n",
    "    df = pd.DataFrame(data={\"words\": words,\n",
    "                            \"category\": cat_labels_for_df,\n",
    "                            newsgroups_train.target_names[0]: coef_0_list,\n",
    "                           newsgroups_train.target_names[1]: coef_1_list,\n",
    "                           newsgroups_train.target_names[2]: coef_2_list,\n",
    "                           newsgroups_train.target_names[3]: coef_3_list})\n",
    "    \n",
    "    df = df.set_index(['category', 'words'])\n",
    "    \n",
    "    return df\n",
    "        \n",
    "## todo: change bigram to words\n",
    "df = make_coefs_table(default_train_dtm, default_cv)\n",
    "bigram_word_cv = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_word_doc_term_matrix = bigram_word_cv.fit_transform(train_data)\n",
    "bigram_df = make_coefs_table(bigram_word_doc_term_matrix, bigram_word_cv)\n",
    "\n",
    "df.round(3)\n",
    "### STUDENT END ###\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By choosing to use largest absolute values for each category, we end up getting some duplicates in the table, but I chose to keep it that way because I thought it was an interesting finding. Most notably, the word \"space\" shows up as one of the strongest signals in every single category; it has a positive coefficient for the \"space\" category, indicating that it's a strong signal that a document that contains it is in that category, and it has a strong negative coefficient for all the other categories. The most surprising features in this table, overall, are \"blood\" as a positive signal for religion, \"bobby\" as a positive signal for atheism, and \"could\" as a negative signal for religion. These signals suggest particularities about the data we're training on which might not extend out if our sample got bigger; perhaps a number of the documents reference a certain \"bobby\" who is active in the atheism conversation, but that signal might not generalize well if we get more documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these features are less obvious than their unigram counterparts. Atheism and graphics in particular have a lot of phrases that seem very run-of-the-mill. The two-word phrases for religion, however, give a pretty strong signal about what kind of documents are included. Phrases like \"such lunacy\" and \"the fbi\" suggest that this might be some kidn of conservative talk radio or something like that, perhaps more religion in culture than in actual content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Try to improve the logistic regression classifier by passing a custom preprocessor to CountVectorizer. The preprocessing function runs on the raw text, before it is split into words by the tokenizer. Your preprocessor should try to normalize the input in various ways to improve generalization. For example, try lowercasing everything, replacing sequences of numbers with a single token, removing various other non-letter characters, and shortening long words. If you're not already familiar with regular expressions for manipulating strings, see https://docs.python.org/2/library/re.html, and re.sub() in particular. With your new preprocessor, how much did you reduce the size of the dictionary?\n",
    "\n",
    "For reference, I was able to improve dev F1 by 2 points.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original logistic regression to start with\n",
    "log_reg = LogisticRegression(C=.5, penalty='l2')\n",
    "log_reg.fit(default_train_dtm, train_labels)\n",
    "dev_preds = log_reg.predict(default_dev_dtm)\n",
    "print(\"Number of features: {}\".format(default_train_dtm.shape[1]))\n",
    "baseline_f1 = metrics.f1_score(dev_labels,\n",
    "                               dev_preds,\n",
    "                               average=\"weighted\")\n",
    "print(\"Baseline F1 score: {:.5f}\".format(baseline_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_regex = re.compile(r\"[\\d]\\s\")\n",
    "big_num_regex = re.compile(r\"[\\d]+\")\n",
    "special_char = re.compile(r\"[^a-zA-Z0-9 ]\")\n",
    "ly = re.compile(r\"([a-z]+)(ly)\")\n",
    "ing = re.compile(r\"([a-z]+)(ing)\")\n",
    "final_s = re.compile(r\"([a-z]+)(s)\")\n",
    "ed = re.compile(r\"([a-z]+)(ed)\")\n",
    "endings = [ly, ing, final_s, ed]\n",
    "### try: common endings (plural, ly, ing, etc..)\n",
    "### feature extraction.text dictionary has ENGLISH_STOP_WORDS - get rid of that\n",
    "\n",
    "# TODO: keep thinking of more things to add to this\n",
    "def better_preprocessor(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(digit_regex, \"digittoken \", s)\n",
    "    s = re.sub(big_num_regex, \"numtoken\", s)\n",
    "    s = re.sub(special_char, \" \", s)\n",
    "    for ending in endings:\n",
    "        s = re.sub(ending, r\"\\1\", s)\n",
    "    for word in s.split():\n",
    "        if len(word) > 12:\n",
    "            s = s.replace(word, word[:12])\n",
    "    return s\n",
    "\n",
    "s = \"Hi my name $is Emily 4 hi numtoken. hi % hi --or--\"\n",
    "print(s)\n",
    "better_preprocessor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_new_cv(preprocessor):\n",
    "    cv = CountVectorizer(preprocessor=preprocessor)\n",
    "    new_dtm = cv.fit_transform(train_data)\n",
    "    new_dev_dtm = cv.transform(dev_data)\n",
    "    ## TODO: add difference between this and orig number of words\n",
    "    print(\"Number of features: {}\".format(new_dtm.shape[1]))\n",
    "    # TODO: replace with grid search\n",
    "    parameters = {'C': [.5] }\n",
    "    log_reg = LogisticRegression(penalty='l2')\n",
    "    lr, f1 = fit_grid_search(log_reg,\n",
    "                    parameters,\n",
    "                    new_dtm,\n",
    "                    new_dev_dtm,\n",
    "                    \"logistic regression\")\n",
    "    print(\"Improvement from baseline: {:.3f}\".format(f1 - baseline_f1))\n",
    "    \n",
    "\n",
    "train_with_new_cv(better_preprocessor)\n",
    "#def better_preprocessor(s):\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#def P5():\n",
    "### STUDENT START ###\n",
    "\n",
    "    \n",
    "### STUDENT END ###\n",
    "#P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) The idea of regularization is to avoid learning very large weights (which are likely to fit the training data, but not generalize well) by adding a penalty to the total size of the learned weights. That is, logistic regression seeks the set of weights that minimizes errors in the training data AND has a small size. The default regularization, L2, computes this size as the sum of the squared weights (see P3, above). L1 regularization computes this size as the sum of the absolute values of the weights. The result is that whereas L2 regularization makes all the weights relatively small, L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. How does this compare to the number of non-zero weights you get with \"l2\"? Now, reduce the size of the vocabulary by keeping only those features that have at least one non-zero weight and retrain a model using \"l2\".\n",
    "\n",
    "Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "Note: The gradient descent code that trains the logistic regression model sometimes has trouble converging with extreme settings of the C parameter. Relax the convergence criteria by setting tol=.01 (the default is .0001).\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    ### STUDENT START ###\n",
    "    def train_lr_return_nonzero_weights(penalty):\n",
    "        lr = LogisticRegression(penalty = penalty)\n",
    "        lr.fit(default_train_dtm, train_labels)\n",
    "        num_nonzero_coefs = np.sum(lr.coef_ != 0)\n",
    "        return lr.coef_\n",
    "    \n",
    "    l1_weights = train_lr_return_nonzero_weights('l1')\n",
    "    l2_weights = train_lr_return_nonzero_weights('l2')\n",
    "    print(\"Number of nonzero weights for LR trained with L1 regularization: {}\".format(np.sum(l1_weights != 0)))\n",
    "    print(\"Number of nonzero weights for LR trained with L2 regularization: {}\".format(np.sum(l2_weights != 0)))\n",
    "    \n",
    "    return l1_weights\n",
    "        \n",
    "    \n",
    "\n",
    "    ### STUDENT END ###\n",
    "l1 = P6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_list = []\n",
    "for i in range(len(l1[0])):\n",
    "    all_weights = [r[i] for r in l1]\n",
    "    if np.sum(all_weights) != 0:\n",
    "        non_zero_list.append(i)\n",
    "print(\"Number of features with non-zero weight: {}\".format(len(non_zero_list)))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm = default_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "just_those_features = dtm[:,non_zero_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l2', tol=.01)\n",
    "lr.fit(just_those_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dtm = default_dev_dtm.toarray()\n",
    "just_those_dev_features = dev_dtm[:, non_zero_list]\n",
    "just_those_dev_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_preds = lr.predict(just_those_dev_features)\n",
    "metrics.f1_score(dev_labels,\n",
    "               dev_preds,\n",
    "               average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plot showing accuracy of the re-trained model vs. the vocabulary size you get when pruning unused features by adjusting the C parameter.\n",
    "\n",
    "keep doing above at different values of C and then plot vocab sizes against accuracies. changing C changes vocabulary because it changes your regularization (so more params are small/zero). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Use the TfidfVectorizer -- how is this different from the CountVectorizer? Train a logistic regression model with C=100.\n",
    "\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "\n",
    "maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "What kinds of mistakes is the model making? Suggest a way to address one particular issue that you see.\n",
    "\n",
    "[4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P7():\n",
    "    ### STUDENT START ###\n",
    "\n",
    "### TODO: show actual prediction, instead of \n",
    "#### ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "tf_dtm = tf_vectorizer.fit_transform(train_data)\n",
    "lr = LogisticRegression(C=100)\n",
    "lr.fit(tf_dtm, train_labels)\n",
    "tf_dtm_dev = tf_vectorizer.transform(dev_data)\n",
    "dev_preds = lr.predict_proba(tf_dtm_dev)\n",
    "\n",
    "def calculate_r(probs, label):\n",
    "    return max(probs) / probs[label]\n",
    "\n",
    "r_values = [calculate_r(probs, dev_labels[i]) for i, probs in enumerate(dev_preds)]\n",
    "top_3_r = sorted(r_values, reverse=True)[:3]\n",
    "idxes_of_top_r = [np.where(r_values==r)[0][0] for r in top_3_r]\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\nR value: {}\".format(top_3_r[i]))\n",
    "    idx_in_set = idxes_of_top_r[i]\n",
    "    print(\"Correct label: {}\".format(dev_labels[idx_in_set]))\n",
    "    print(\"Predicted probabilities: {}\".format(dev_preds[idx_in_set]))\n",
    "    print(\"Text: {}\".format(dev_data[idx_in_set]))\n",
    "    ### STUDENT END ###\n",
    "#P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "The TfidVectorizer is different from the CountVectorizer in that instead of representing a document by the counts of words contained, it represents it by the counts of words contained times the inverse document frequency. This measure scales the count of words in the document by the number of documents that the word appears in, such that words that appear frequently across documents (and are, therefore, less meaningful from a classification perspective) won't have as high of values.\n",
    "\n",
    "## TODO: check if other people's look similar here...\n",
    "### todo change: so it uses the word \"ftp\" which is a very computer-y word. \n",
    "### our religion stuff is really violent...?\n",
    "In terms of mistakes the model is making, it seems like the documents with the highest R-squared are the ones where a particular word with an extreme tfidf is used in a different context. For instance, in the last example here, the word \"space\" is used, which should suggest the \"space\" label, but in this case is a news story. I'm not sure why label three was the correct label for the other two examples, though - honestly, I wonder if they are mislabeled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) EXTRA CREDIT\n",
    "\n",
    "Try implementing one of your ideas based on your error analysis. Use logistic regression as your underlying model.\n",
    "\n",
    "- [1 pt] for a reasonable attempt\n",
    "- [2 pts] for improved performance\n",
    "\n",
    "### people just cut out some of these problematic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
