{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Digit Classification with KNN and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll implement your own image recognition system for classifying digits. Read through the code and the instructions carefully and add your own code where indicated. Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but <b> please prepare your own write-up (with your own code). </b>\n",
    "\n",
    "If you're interested, check out these links related to digit recognition:\n",
    "\n",
    "Yann Lecun's MNIST benchmarks: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Stanford Streetview research and data: http://ufldl.stanford.edu/housenumbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOTE: this project was done with sklearn 0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Notice that we are splitting the data into training, development, and test. We also have a small subset of the training data called mini_train_data and mini_train_labels that you should use in all the experiments below, unless otherwise noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digit data either from mldata.org, or once downloaded to data_home, from disk. The data is about 53MB so this cell\n",
    "# should take a while the first time your run it.\n",
    "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
    "X, Y = mnist.data, mnist.target\n",
    "\n",
    "# Rescale grayscale values to [0,1].\n",
    "X = X / 255.0\n",
    "\n",
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "print('data shape: ', X.shape)\n",
    "print('label shape:', Y.shape)\n",
    "\n",
    "# Set some variables to hold test, dev, and training data.\n",
    "test_data, test_labels = X[61000:], Y[61000:]\n",
    "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
    "train_data, train_labels = X[:60000], Y[:60000]\n",
    "mini_train_data, mini_train_labels = X[:1000], Y[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Create a 10x10 grid to visualize 10 examples of each digit. Python hints:\n",
    "\n",
    "- plt.rc() for setting the colormap, for example to black and white\n",
    "- plt.subplot() for creating subplots\n",
    "- plt.imshow() for rendering a matrix\n",
    "- np.array.reshape() for reshaping a 1D feature vector into a 2D matrix (for rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P1(num_examples=10):\n",
    "\n",
    "### STUDENT START ###\n",
    "def create_examples_dict(train_labels, num_examples):\n",
    "    examples_dict = {num: [] for num in range(10)}\n",
    "    sets_found = set()\n",
    "    for i, label in enumerate(train_labels):\n",
    "        if len(sets_found) == num_examples:\n",
    "            break\n",
    "        if len(examples_dict[label]) < num_examples:\n",
    "            examples_dict[label].append(i)\n",
    "        else:\n",
    "            sets_found.add(label)\n",
    "    return examples_dict\n",
    "\n",
    "def plot_examples(examples_dict, train_data, num_examples):\n",
    "    # loop over subplots and access correct matrix\n",
    "    # for each value in each dictionary entry\n",
    "    f, axarr = plt.subplots(len(examples_dict),num_examples, figsize=(15,15))\n",
    "    for i in range(len(examples_dict)):\n",
    "        for j in range(num_examples):\n",
    "            desired_matrix = mini_train_data[examples_dict[i][j]]\n",
    "            im_to_show = np.reshape(desired_matrix, (28, 28))\n",
    "            axarr[i,j].imshow(im_to_show, cmap=plt.cm.Greys)\n",
    "            axarr[i,j].axis('off')\n",
    "\n",
    "number_of_examples = 10\n",
    "ex_dict = create_examples_dict(mini_train_labels, number_of_examples)\n",
    "plot_examples(ex_dict, mini_train_data, number_of_examples)\n",
    "### STUDENT END ###\n",
    "\n",
    "#P1(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Evaluate a K-Nearest-Neighbors model with k = [1,3,5,7,9] using the mini training set. Report accuracy on the dev set. For k=1, show precision, recall, and F1 for each label. Which is the most difficult digit?\n",
    "\n",
    "- KNeighborsClassifier() for fitting and predicting\n",
    "- classification_report() for producing precision, recall, F1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P2(k_values):\n",
    "\n",
    "### STUDENT START ###\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "\n",
    "def train_k_neighbors(k_values):\n",
    "    for k in k_values:\n",
    "        k_neighbors = KNeighborsClassifier(k)\n",
    "        k_neighbors.fit(mini_train_data, mini_train_labels)\n",
    "        dev_predictions = k_neighbors.predict(dev_data)\n",
    "        print(\"Accuracy when k={}: \".format(k), np.sum(dev_predictions==dev_labels) / 1000)\n",
    "        if k==1:\n",
    "            print(\"\\nClassification report when k=1:\")\n",
    "            print(classification_report(dev_labels, dev_predictions))\n",
    "\n",
    "train_k_neighbors(k_values)    \n",
    "### STUDENT END ###\n",
    "\n",
    "#k_values = [1, 3, 5, 7, 9]\n",
    "#P2(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: The most difficult digit for precision is 3, which means that of all the digits the model predicts to be 3's, a smaller percentage of those predictions are correct compared to the other labels. The most difficult digit for recall is 2, which means that the model incorrectly predicts on more 2's than it does on other labels. Overall, the digit with the lowest f1 score (combination of precision and recall) is 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Using k=1, report dev set accuracy for the training set sizes below. Also, measure the amount of time needed for prediction with each training size.\n",
    "\n",
    "- time.time() gives a wall clock value you can use for timing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P3(train_sizes, accuracies):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25000]\n",
    "accuracies = []\n",
    "\n",
    "def train_k_neighbors_by_size(train_sizes, accuracies):\n",
    "    for size in train_sizes:\n",
    "        k_neighbors = KNeighborsClassifier(1)\n",
    "        k_neighbors.fit(train_data[:size], train_labels[:size])\n",
    "        start_time = time.time()\n",
    "        dev_predictions = k_neighbors.predict(dev_data)\n",
    "        end_time = time.time()\n",
    "        accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "        accuracies.append(accuracy)\n",
    "        prediction_time = end_time - start_time\n",
    "        print(\"Training set size: {}\".format(size))\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Time taken for 1000 predictions: {:.2f} seconds\\n\".format(prediction_time))\n",
    "\n",
    "train_k_neighbors_by_size(train_sizes, accuracies)\n",
    "### TODO: use model.score for accuracy \n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25000]\n",
    "#accuracies = []\n",
    "#P3(train_sizes, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Fit a regression model that predicts accuracy from training size. What does it predict for n=60000? What's wrong with using regression here? Can you apply a transformation that makes the predictions more reasonable?\n",
    "\n",
    "- Remember that the sklearn fit() functions take an input matrix X and output vector Y. So each input example in X is a vector, even if it contains only a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P4():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "# turn train sizes list into array and reshape into 2-d matrix\n",
    "def fit_accuracy_predictor(train_sizes_array, \n",
    "                           accuracies, \n",
    "                           value_to_predict,\n",
    "                           message):\n",
    "    # create and fit linear regression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train_sizes_array, accuracies)\n",
    "\n",
    "    print(\"Prediction for training size 60,000 with {}: {:.2f}\".format(message,\n",
    "                                                                       lr.predict(value_to_predict)[0]))\n",
    "\n",
    "train_sizes_array = np.asarray(train_sizes)\n",
    "train_sizes_array = np.reshape(train_sizes_array, (len(train_sizes_array), 1))\n",
    "\n",
    "sixty_thousand = np.array([60000]).reshape(-1,1)\n",
    "\n",
    "# original linear model fit\n",
    "fit_accuracy_predictor(train_sizes_array, \n",
    "                       accuracies, \n",
    "                       sixty_thousand, \n",
    "                       \"no transformation\")\n",
    "\n",
    "# trying again with a log transform\n",
    "fit_accuracy_predictor(np.log(train_sizes_array),\n",
    "                      accuracies,\n",
    "                      np.log(sixty_thousand),\n",
    "                      \"log transform of training size\")\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: When we regress accuracy on training examples, the model predicts that a training size of 60,000 will lead to an accuracy of 1.243, which is not possible, since the most highest accuracy a model can have is 1 (100% accurate). We need to apply a transformation to the output variable such that the accuracy values will be modeled as approaching 1. A good way of doing this is applying a log transformation to the training size. This effectively models the way that accuracy gains decrease as we continue to up the training size. We still get a prediction that is slightly over 1, but we can effectively understand this to mean that the regression predicts perfect or near-perfect accuracy at 60,000 training rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a 1-NN and output a confusion matrix for the dev data. Use the confusion matrix to identify the most confused pair of digits, and display a few example mistakes.\n",
    "\n",
    "- confusion_matrix() produces a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P5():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "# train, fit, and predict on the classifier\n",
    "k_neighbors = KNeighborsClassifier(1)\n",
    "k_neighbors.fit(mini_train_data, mini_train_labels)\n",
    "dev_predictions = k_neighbors.predict(dev_data)\n",
    "\n",
    "# show default confusion matrix\n",
    "cm = confusion_matrix(dev_labels, dev_predictions)\n",
    "print(cm)\n",
    "\n",
    "# show prettier confusion matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.imshow(cm, cmap=plt.cm.Greys)\n",
    "\n",
    "# find all occurrences where a true 4 is predicted as a 9\n",
    "true_4s = np.where(dev_labels==4)\n",
    "predicted_9s = np.where(dev_predictions==9)\n",
    "true_4s_predicted_9s = np.intersect1d(true_4s, predicted_9s)\n",
    "\n",
    "# display the examples\n",
    "f, axarr = plt.subplots(1,5, figsize=(6, 6))\n",
    "plt.title(\"True 4s confused for 9s\", loc='left')\n",
    "for i, idx in enumerate(true_4s_predicted_9s[:5]):\n",
    "    desired_matrix = dev_data[idx]\n",
    "    im_to_show = np.reshape(desired_matrix, (28, 28))\n",
    "    axarr[i].imshow(im_to_show)\n",
    "    axarr[i].axis('off')\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) A common image processing technique is to smooth an image by blurring. The idea is that the value of a particular pixel is estimated as the weighted combination of the original value and the values around it. Typically, the blurring is Gaussian -- that is, the weight of a pixel's influence is determined by a Gaussian function over the distance to the relevant pixel.\n",
    "\n",
    "Implement a simplified Gaussian blur by just using the 8 neighboring pixels: the smoothed value of a pixel is a weighted combination of the original value and the 8 neighboring values. Try applying your blur filter in 3 ways:\n",
    "- preprocess the training data but not the dev data\n",
    "- preprocess the dev data but not the training data\n",
    "- preprocess both training and dev data\n",
    "\n",
    "Note that there are Guassian blur filters available, for example in scipy.ndimage.filters. You're welcome to experiment with those, but you are likely to get the best results with the simplified version I described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P6():\n",
    "    \n",
    "### STUDENT START ###\n",
    "\n",
    "def apply_gaussian_blur(orig_matrix):\n",
    "    orig_matrix = np.reshape(orig_matrix, (28, 28))\n",
    "    new_matrix = np.copy(orig_matrix)\n",
    "    \n",
    "    it = np.nditer(new_matrix, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        # get value for the desired pixel in old matrix\n",
    "        desired_pixel_orig = orig_matrix[it.multi_index]\n",
    "\n",
    "        row_idx = it.multi_index[0]\n",
    "        column_idx = it.multi_index[1]\n",
    "        \n",
    "        # get values for the surrounding pixels in old matrix\n",
    "        surrounding_pixels = []\n",
    "        \n",
    "        # loop over the rows\n",
    "        for i in range(row_idx - 1, row_idx + 2):\n",
    "            # loop over the columns\n",
    "            for j in range(column_idx - 1, column_idx + 2):\n",
    "                # this if statement handles LITERAL edge cases\n",
    "                # as in, the edges of the image\n",
    "                if 0 <= i <= 27 and 0 <= j <= 27:\n",
    "                    if not (i, j) == it.multi_index:\n",
    "                        surrounding_pixels.append(orig_matrix[i, j])\n",
    "\n",
    "        # half times original value, half the weight of surrounding values\n",
    "        new_pixel_value = .5*(desired_pixel_orig) + .5*(np.mean(surrounding_pixels))\n",
    "        \n",
    "        # replace it[0] with desired value\n",
    "        it[0] = new_pixel_value\n",
    "\n",
    "        it.iternext()\n",
    "    \n",
    "        #reshape new matrix to original dimensions\n",
    "        new_matrix = np.reshape(new_matrix, (784,))\n",
    "    return new_matrix\n",
    "\n",
    "# pre-process the train and dev data\n",
    "blurred_train_data = [apply_gaussian_blur(x) for x in mini_train_data]\n",
    "blurred_dev_data = [apply_gaussian_blur(x) for x in dev_data]\n",
    "\n",
    "def train_and_predict(train_option, dev_option, message):\n",
    "    k_neighbors = KNeighborsClassifier(1)\n",
    "    k_neighbors.fit(train_option, mini_train_labels)\n",
    "    dev_predictions = k_neighbors.predict(dev_option)\n",
    "    accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "    print(\"Accuracy with {}: {}\".format(message, accuracy))\n",
    "\n",
    "# compare 1-nn with...\n",
    "\n",
    "# no blurring\n",
    "train_and_predict(mini_train_data, dev_data, \"no blurring\")\n",
    "\n",
    "# train blurred, dev not blurred\n",
    "train_and_predict(blurred_train_data, dev_data, \"blurring for train, not dev\")\n",
    "\n",
    "# train not blurred, dev blurred\n",
    "train_and_predict(mini_train_data, blurred_dev_data, \"blurring for dev, not train\")\n",
    "\n",
    "# both blurred\n",
    "train_and_predict(blurred_train_data, blurred_dev_data, \"blurring for both train and dev\")\n",
    "### STUDENT END ###\n",
    "\n",
    "#P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: The model gets the best accuracy when the training data is blurred, but the dev data isn't. This makes sense to me based on what I know about using dropout in neural networks to prevent overfitting. It helps to train on blurred data, so that you prevent overfitting to the small nuances of your training cases, but it's helpful to keep as much information as possible about the cases you want to predict on. However, the accuracy difference between blurring on dev and not blurring on dev is relatively small, so it seems this could work well either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Fit a Naive Bayes classifier and report accuracy on the dev data. Remember that Naive Bayes estimates P(feature|label). While sklearn can handle real-valued features, let's start by mapping the pixel values to either 0 or 1. You can do this as a preprocessing step, or with the binarize argument. With binary-valued features, you can use BernoulliNB. Next try mapping the pixel values to 0, 1, or 2, representing white, grey, or black. This mapping requires MultinomialNB. Does the multi-class version improve the results? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P7():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "bernoulli_nb = BernoulliNB(binarize = 0.33)\n",
    "bernoulli_nb.fit(mini_train_data, mini_train_labels)\n",
    "dev_predictions = bernoulli_nb.predict(dev_data)\n",
    "accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "print(\"Accuracy for Bernoulli: {}\".format(accuracy))\n",
    "\n",
    "def preprocess_for_multinomial(image_matrix):\n",
    "    initial_matrix = (image_matrix > .33).astype(int)\n",
    "    black_values_matrix = (image_matrix > .66).astype(int)\n",
    "    final_matrix = initial_matrix + black_values_matrix\n",
    "    return final_matrix\n",
    "\n",
    "pre_processed_train_data = []\n",
    "pre_processed_dev_data = []\n",
    "\n",
    "for image_matrix in mini_train_data:\n",
    "    pre_processed_train_data.append(preprocess_for_multinomial(image_matrix))\n",
    "\n",
    "for image_matrix in dev_data:\n",
    "    pre_processed_dev_data.append(preprocess_for_multinomial(image_matrix))\n",
    "    \n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(pre_processed_train_data, mini_train_labels)\n",
    "dev_predictions = bernoulli_nb.predict(pre_processed_dev_data)\n",
    "accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "print(\"Accuracy for Multinomial: {}\".format(accuracy))\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: Switching to Multinomial does not improve results. This is most likely because the additional signal created by separating out grey from black pixels isn't really important for determining what label is right for an image. From a number-identifying perspective, what is important is that there is some value in the pixel, indicating the presence of a pen stroke. A certain pixel being more grey than black might have as much to do with the image processing as it does with what the original written digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Use GridSearchCV to perform a search over values of alpha (the Laplace smoothing parameter) in a Bernoulli NB model. What is the best value for alpha? What is the accuracy when alpha=0? Is this what you'd expect?\n",
    "\n",
    "- Note that GridSearchCV partitions the training data so the results will be a bit different than if you used the dev data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P8(alphas):\n",
    "\n",
    "### STUDENT START ###\n",
    "    nb = BernoulliNB()\n",
    "    grid_search = GridSearchCV(nb, alphas)\n",
    "    grid_search.fit(mini_train_data, mini_train_labels)\n",
    "    return grid_search\n",
    "\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "nb = P8(alphas)\n",
    "\n",
    "print(nb.best_params_)\n",
    "print()\n",
    "for i in range(len(alphas['alpha'])):  \n",
    "    print(nb.cv_results_['params'][i])\n",
    "    print(\"Accuracy: {}\".format(nb.cv_results_['mean_test_score'][i]))\n",
    "    print()\n",
    "    \n",
    "### STUDENT END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: The best value for alpha is .1, as the classifier has an accuracy of .821. The accuracy when alpha = 0 is .803, which is worse. It makes sense that the value with no smoothing would be worse, because smoothing helps prevent the classifier from overfitting to its training data. We would expect the smoothing effect to be less important as the training size increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) Try training a model using GuassianNB, which is intended for real-valued features, and evaluate on the dev data. You'll notice that it doesn't work so well. Try to diagnose the problem. You should be able to find a simple fix that returns the accuracy to around the same rate as BernoulliNB. Explain your solution.\n",
    "\n",
    "Hint: examine the parameters estimated by the fit() method, theta\\_ and sigma\\_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P9():\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "def train_gaussian(mini_train_data,\n",
    "                   mini_train_labels,\n",
    "                   smoothing):\n",
    "    gaussian_nb = GaussianNB(var_smoothing = smoothing)\n",
    "    gaussian_nb.fit(mini_train_data, mini_train_labels)\n",
    "    return gaussian_nb\n",
    "\n",
    "def get_accuracy_for_gaussian(model,\n",
    "                              dev_data,\n",
    "                              dev_labels,\n",
    "                              smoothing):\n",
    "    dev_predictions = model.predict(dev_data)\n",
    "    accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "    print(\"Accuracy for Gaussian with {} value for smoothing: {}\".format(smoothing,\n",
    "                                                                         accuracy))\n",
    "\n",
    "# the default smoothing value is 1e^-09. this is close\n",
    "default_smooth = .0001234\n",
    "gaussian_nb = train_gaussian(mini_train_data,\n",
    "                            mini_train_labels,\n",
    "                            default_smooth)\n",
    "get_accuracy_for_gaussian(gaussian_nb,\n",
    "                          dev_data,\n",
    "                          dev_labels,\n",
    "                          default_smooth)\n",
    "orig_sigmas = [np.mean(pixel) for pixel in gaussian_nb.sigma_[0]]\n",
    "\n",
    "\n",
    "# I tested a variety of smooth values and\n",
    "# this one was best\n",
    "smooth_value = .08\n",
    "gaussian_nb_smoothed = train_gaussian(mini_train_data,\n",
    "                                     mini_train_labels,\n",
    "                                     smooth_value)\n",
    "get_accuracy_for_gaussian(gaussian_nb_smoothed,\n",
    "                          dev_data,\n",
    "                          dev_labels,\n",
    "                          smooth_value)\n",
    "\n",
    "new_sigmas = [np.mean(pixel) for pixel in gaussian_nb_smoothed.sigma_[0]]\n",
    "\n",
    "bins = np.linspace(0, .3, 30)\n",
    "plt.hist(orig_sigmas, bins, alpha=0.5, label='Original Sigmas')\n",
    "plt.hist(new_sigmas, bins, alpha=0.5, label='Smoothed Sigmas')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Change in Sigma Values for All Pixels in Examples with Label 0\")\n",
    "plt.xlabel(\"Sigma Value\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "plt.show()\n",
    "\n",
    "change_in_sigma = [new_sigmas[i] - orig_sigmas[i] for i in range(len(orig_sigmas))]\n",
    "print(\"Average increase in sigma: {}\".format(np.mean(change_in_sigma)))\n",
    "### STUDENT END ###\n",
    "\n",
    "#gnb = P9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:  A Gaussian model is expecting a normal distribution for the features, whereas for these features, the distribution is not normal at all, and is highly skewed towards 0. As a result, when the model tries to fit the values for each feature to a normal distribution, that distribution ends up being centered very close to 0, with a small variance (small sigmas). Then, when the model sees values for those features that are several standard deviations from the mean, it doesn't know what to do, and guesses randomly. Changing the value of the smoothing parameter causes the model to train such that it finds larger values for sigmas for each feature. With larger variances, the normal distribution for each feature is wider, so more data points fall in the part of the distribution where the model can accurately recognize the value and identify it. The plot shows how much bigger the sigmas are for the pixels in the 0-labeled examples once we add a higher smoothing parameter to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) Because Naive Bayes is a generative model, we can use the trained model to generate digits. Train a BernoulliNB model and then generate a 10x20 grid with 20 examples of each digit. Because you're using a Bernoulli model, each pixel output will be either 0 or 1. How do the generated digits compare to the training digits?\n",
    "\n",
    "- You can use np.random.rand() to generate random numbers from a uniform distribution\n",
    "- The estimated probability of each pixel is stored in feature\\_log\\_prob\\_. You'll need to use np.exp() to convert a log probability back to a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P10(num_examples):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "def train_bernoulli_nb(binarize):\n",
    "    bernoulli_nb = BernoulliNB(binarize = binarize)\n",
    "    bernoulli_nb.fit(mini_train_data, mini_train_labels)\n",
    "    dev_predictions = bernoulli_nb.predict(dev_data)\n",
    "    accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "    print(\"Accuracy for Bernoulli: {}\".format(accuracy))\n",
    "    return bernoulli_nb\n",
    "\n",
    "def generate_digits(examples_per_num, nb):\n",
    "    images_to_show = []\n",
    "    for item in nb.feature_log_prob_:\n",
    "        images_per_digit = []\n",
    "        for i in range(examples_per_num):\n",
    "            generated_digit = []\n",
    "            for pixel_prob in np.exp(item):\n",
    "                pixel_val = 0\n",
    "                num = np.random.rand()\n",
    "                if num <= pixel_prob:\n",
    "                    pixel_val = 1\n",
    "                generated_digit.append(pixel_val)\n",
    "            im_to_show = np.reshape(generated_digit, (28, 28))\n",
    "            images_per_digit.append(im_to_show)\n",
    "        images_to_show.append(images_per_digit)\n",
    "    return images_to_show\n",
    "\n",
    "def plot_generated_digits(images_to_show):\n",
    "    f, axarr = plt.subplots(10,20, figsize=(15,15))\n",
    "    for i, examples in enumerate(images_to_show):\n",
    "        for j, example in enumerate(examples):\n",
    "            im_to_show = np.reshape(example, (28, 28))\n",
    "            axarr[i,j].imshow(im_to_show, cmap=plt.cm.Greys)\n",
    "            axarr[i,j].axis('off')\n",
    "\n",
    "b_nb = train_bernoulli_nb(.33)\n",
    "im_to_show = generate_digits(20, b_nb)\n",
    "plot_generated_digits(im_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: None of the generated images actually look like something someone woud have drawn. That's because each pixel value was determined independently of its neighboring values, based solely on the average value of that pixel over the training examples. With this method, the probabilities ensure that, over 784 pixels, we are likely to activate at least some of the pixels that are typically used for the given number, so we get the overall impression of what number it was. In a real training example, someone draws a digit using a smooth pen stroke, so the likelihood of a certain pixel to be black is very related to the likelihood that its neighbors are black. Since we don't take into account neighboring pixel values in generating these images, they don't mimic the look of a realistic pen stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11) Remember that a strongly calibrated classifier is rougly 90% accurate when the posterior probability of the predicted class is 0.9. A weakly calibrated classifier is more accurate when the posterior is 90% than when it is 80%. A poorly calibrated classifier has no positive correlation between posterior and accuracy.\n",
    "\n",
    "Train a BernoulliNB model with a reasonable alpha value. For each posterior bucket (think of a bin in a histogram), you want to estimate the classifier's accuracy. So for each prediction, find the bucket the maximum posterior belongs to and update the \"correct\" and \"total\" counters.\n",
    "\n",
    "How would you characterize the calibration for the Naive Bayes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB(binarize = 0.33, alpha=.1)\n",
    "bernoulli_nb.fit(mini_train_data, mini_train_labels)\n",
    "dev_predictions = bernoulli_nb.predict(dev_data)\n",
    "accuracy = np.sum(dev_predictions==dev_labels) / 1000\n",
    "print(\"Accuracy for Bernoulli: {}\".format(accuracy))\n",
    "    \n",
    "### STUDENT START ###\n",
    "\n",
    "pred_probs = bernoulli_nb.predict_proba(dev_data)\n",
    "                \n",
    "### STUDENT END ###\n",
    "\n",
    "buckets = [0.5, 0.9, 0.999, 0.99999, 0.9999999, 0.999999999, 0.99999999999, 0.9999999999999, 1.0]\n",
    "\n",
    "def determine_calibration(prob_predictions, labels, buckets):\n",
    "    bucket_dict = {b: [] for b in buckets}\n",
    "    for i, prediction_probs in enumerate(prob_predictions):\n",
    "        prob = np.amax(prediction_probs)\n",
    "        pred = np.argmax(prediction_probs)\n",
    "        accurate_pred_ind = 0\n",
    "        if pred == labels[i]:\n",
    "            accurate_pred_ind = 1\n",
    "        for b in buckets:\n",
    "            # included predictions at given prob +- .05\n",
    "            # to get more data points\n",
    "            # I didn't want to include everything between say, .05 and .09\n",
    "            # because that bin seems really wide\n",
    "            if (b - .05) <= prob < (b + .05):\n",
    "                bucket_dict[b].append(accurate_pred_ind)\n",
    "                break\n",
    "    return bucket_dict\n",
    "\n",
    "final_bucket_dict = determine_calibration(pred_probs,\n",
    "                                           dev_labels,\n",
    "                                           buckets)\n",
    "\n",
    "for bucket, results in final_bucket_dict.items():\n",
    "    if len(results) > 0:\n",
    "        accuracy = np.mean(results)\n",
    "        print(\"True accuracy for predictions with probability {}: {}\".format(bucket, accuracy))\n",
    "        print(\"Number of predictions: {}\".format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: The Naive Bayes classifier appears not to be callibrated at all. While 66.7% of its predictions with probability .5 (+- .05) are in fact, correct, only 45.5% of its predictions with probability .9 (+- .05) are correct, and only 85% of its predictions with probability .999 (+- .035 are correct. If this classifier were well-calibrated, we would see its accuracy go up in lock-step with its prediction probability, and ideally, we'd see its accuracy being very similar to its prediction probability. Here, neither seems to be the case, although we don't see very much data in its smaller buckets to confirm the trend. It appears that the model is typically extremely confident (around 99.9% confident for 94% of its data) in its predictions, even though in reality it is only correct about 83% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(12) EXTRA CREDIT\n",
    "\n",
    "Try designing extra features to see if you can improve the performance of Naive Bayes on the dev set. Here are a few ideas to get you started:\n",
    "- Try summing the pixel values in each row and each column.\n",
    "- Try counting the number of enclosed regions; 8 usually has 2 enclosed regions, 9 usually has 1, and 7 usually has 0.\n",
    "\n",
    "Make sure you comment your code well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def P12():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "# trying an ensemble of NB's and K-nns\n",
    "# and then have them vote for which label\n",
    "# if there's no consensus across votes, they'll pick randomly from the 3 options\n",
    "\n",
    "def train_model(model,\n",
    "                mini_train_data, \n",
    "                mini_train_labels):\n",
    "    return model.fit(mini_train_data, mini_train_labels)\n",
    "\n",
    "def predict_on_model(model,\n",
    "                     dev_data, \n",
    "                     dev_labels):\n",
    "    return model.predict(dev_data)\n",
    "\n",
    "model_dict = {\"bernoulli_alpha\": BernoulliNB(alpha=.1),\n",
    "             \"bernoulli\": BernoulliNB(),\n",
    "             \"gaussian_smoothed\": GaussianNB(var_smoothing=.08),\n",
    "             \"knn_1\": KNeighborsClassifier(1),\n",
    "             \"knn_3\": KNeighborsClassifier(3),\n",
    "             \"knn_5\": KNeighborsClassifier(5),\n",
    "             \"knn_5_distance\": KNeighborsClassifier(5, weights=\"distance\"),\n",
    "             \"knn_7_distance\": KNeighborsClassifier(7, weights=\"distance\")}\n",
    "\n",
    "\n",
    "model_predictions_dict = {}\n",
    "\n",
    "# loop over models to train them and predict on them\n",
    "# add their predictions to the model predictions dict\n",
    "# we use this dictionary so that we can train each model just once,\n",
    "# even if we want to use that model in several ensembles\n",
    "for model_name, model_function in model_dict.items():\n",
    "    trained_model = train_model(model_function, mini_train_data, mini_train_labels)\n",
    "    model_predictions = predict_on_model(trained_model, dev_data, dev_labels)\n",
    "    model_predictions_dict[model_name] = model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_include = [ [\"bernoulli_alpha\", \n",
    "                     \"gaussian_smoothed\", \n",
    "                     \"knn_1\"],\n",
    "                    [\"bernoulli_alpha\", \n",
    "                     \"gaussian_smoothed\", \n",
    "                     \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5\"],\n",
    "                    [\"bernoulli_alpha\", \n",
    "                     \"gaussian_smoothed\", \n",
    "                     \"bernoulli\",\n",
    "                     \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5\"],\n",
    "                    [\"bernoulli_alpha\", \n",
    "                     \"gaussian_smoothed\", \n",
    "                     \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5_distance\"],\n",
    "                     [\"bernoulli_alpha\", \n",
    "                     \"gaussian_smoothed\", \n",
    "                     \"knn_1\",\n",
    "                     \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5_distance\"],\n",
    "                     [\"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5_distance\"],\n",
    "                    [\"knn_1\",\n",
    "                     \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5_distance\"],\n",
    "                    [\"bernoulli_alpha\", \n",
    "                     \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5_distance\"],\n",
    "                     [\"bernoulli_alpha\", \n",
    "                     \"knn_1\",\n",
    "                      \"knn_1\",\n",
    "                     \"knn_3\",\n",
    "                     \"knn_5_distance\"]]\n",
    "\n",
    "# now, for a given model ensemble\n",
    "# gather all predictions\n",
    "for model_combo in models_to_include:\n",
    "    predictions_for_ensemble = []\n",
    "                     \n",
    "    for model_name in model_combo:\n",
    "        predictions_for_ensemble.append(model_predictions_dict[model_name])\n",
    "    # get all the predictions \n",
    "                     \n",
    "    # execute model voting,\n",
    "    # determine ensemble accuracy\n",
    "    accuracy_count = []\n",
    "    for i in range(len(dev_data)):\n",
    "        preds_for_i = [preds[i] for preds in predictions_for_ensemble]\n",
    "        # this shuffle is how we ensure that random prediction will get chosen\n",
    "        # if there isn't consensus\n",
    "        np.random.shuffle(preds_for_i)\n",
    "        counts = np.bincount(preds_for_i)\n",
    "        voted_prediction = np.argmax(counts)\n",
    "        if voted_prediction == dev_labels[i]:\n",
    "            accuracy_count.append(1)\n",
    "        else:\n",
    "            accuracy_count.append(0)\n",
    "\n",
    "    accuracy_for_ensemble = np.sum(accuracy_count) / 1000\n",
    "    print(\"Tried ensemble with these models: {}\".format(model_combo))\n",
    "    print(\"Ensemble Accuracy: {}\\n\".format(accuracy_for_ensemble))\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P12()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of new features, I decided to create an ensemble of models and have them predict via a vote. I wanted them to all train on the same mini_train_data, and I didn't use the blurring. I tried a bunch of different combinations of Naive Bayes and k-nn models with different hyperparameters. A subset of them are shown here. My best ensemble is the last one shown here. The accuracy I got on the mini train data, .895, is higher than what I got for any model trained on this set, other than the ones I did blurring on. \n",
    "\n",
    "A couple things that I learned along the way:\n",
    "- For k-nn 5, changing the weights from \"uniform\" to \"distance\" really made a difference in the overall accuracy, whereas it didn't make much of a difference for k-nn 3. This makes sense, because it seems likely that once you are considering 5 training examples for classification, there's a reasonable chance that a couple of them are significantly further away and shouldn't be counted as much.\n",
    "- k-nn 7 made things worse whether I switched it to \"distance\" or not. I suspect that this is a function of the small training size, and would change if we trained on more data (because there would likely be more examples that were true fits for the given dev example that would end up being in the nearest 7). \n",
    "- Because the k-nn models did better than the Naive Bayes models in general, ensembles that were more skewed towards k-nn models (even if some of the k-nn models were duplicates) did better than those that had more Naive Bayes models. That said, the best combinations did have some representation from Naive Bayes, which suggests that maybe the Naive Bayes models, while worse overall, are catching some of the cases that the k-nns aren't as good at. \n",
    "- Overall, this ensemble isn't that much better than just using a k-nn 1 (.895 vs .888). So, depending on the situation, it likely wouldn't be worth the additional computational complexity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
